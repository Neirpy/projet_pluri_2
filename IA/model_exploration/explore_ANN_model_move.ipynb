{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exploration des meilleurs modèles\n",
    "\n",
    "### Avec les explorations des modèles précédents, nous avons pu constater que :\n",
    "\n",
    "De nombreux modèles présentent d'excellentes performances : Random Forest, Gradient Boosting, SVM, KNN, etc. Que ce soit pour la classification de la vitesse ou des déplacements, les résultats sont très satisfaisants.\n",
    "\n",
    "L’idée initiale était d’explorer les hyperparamètres de ces modèles afin d’en améliorer les performances, mais au vu des résultats obtenus, cela ne semble pas nécessaire pour le moment.\n",
    "\n",
    "Cependant, nous allons tout de même explorer les modèles de type ANN (Artificial Neural Networks), dans le but d’augmenter le taux de prédictions \"neutres\" pour les mouvements, et ainsi réduire les cas où des mouvements qui devraient être classés comme neutres ne le sont pas."
   ],
   "id": "258d23244deeada"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ANN pour la classification des déplacements",
   "id": "eac0deae07e2b7c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Train split des données pour la classification des déplacements.",
   "id": "9a844fc1fd68457b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../preprocessing_data/move_preprocess.csv')\n",
    "\n",
    "X = data.drop('action', axis=1)\n",
    "y = data['action']\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(le.classes_)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val= train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "8915c53c1841e2c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Nombre de colonne de X\n",
    "\n",
    "X_train.shape[1]\n",
    "\n"
   ],
   "id": "faa4fb4e653ec922"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Recherche des meilleurs hyperparamètres pour l'ANN avec RandomizedSearchCV",
   "id": "b755139cef40c9db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import numpy as np\n",
    "\n",
    "def create_ann_model(input_dim, nb_outputs, nb_layers=2, nb_neurons=8, dropout_rate=0.2, activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_dim,)))\n",
    "    model.add(Dense(nb_neurons, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for _ in range(nb_layers - 1):\n",
    "        model.add(Dense(nb_neurons, activation=activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(nb_outputs, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "clf = KerasClassifier(\n",
    "    model=create_ann_model,\n",
    "    input_dim=X_train.shape[1],\n",
    "    nb_outputs=len(np.unique(y_train)),\n",
    "    verbose=0\n",
    ")"
   ],
   "id": "fa62846d0094b2de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "param_dist = {\n",
    "    'model__nb_layers': [2, 3, 5],\n",
    "    'model__nb_neurons': [8, 16, 32, 64],\n",
    "    'model__dropout_rate': [0.2, 0.3, 0.5],\n",
    "    'model__activation': ['relu', 'tanh'],\n",
    "    'epochs': [30, 50, 100],\n",
    "    'batch_size': [32, 64, 128],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=clf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    scoring='accuracy',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ],
   "id": "6af5a67ada0469cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Meilleurs hyperparamètres :\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "print(\"Meilleur score (validation croisée) :\")\n",
    "print(random_search.best_score_)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = random_search.predict(X_val)\n",
    "print(\"Score sur validation :\", accuracy_score(y_val, y_pred))"
   ],
   "id": "1d3920d6f8b4a420"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Recherche plus exhaustive des hyperparamètres pour l'ANN avec GridSearchCV",
   "id": "16b806cd6a52b189"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Une recherche plus exhaustive des hyperparamètres a été réalisée pour le modèle ANN, en utilisant GridSearchCV. Cette recherche s'est concentrée autour des meilleurs hyperparamètres identifiés précédemment par RandomizedSearchCV.",
   "id": "9422c506761a1db3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "\n",
    "clf = KerasClassifier(\n",
    "    model=create_ann_model,\n",
    "    input_dim=X_train.shape[1],\n",
    "    nb_outputs=len(np.unique(y_train)),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'model__nb_neurons': [16, 32],\n",
    "    'model__nb_layers': [2, 3, 5],\n",
    "    'model__activation': ['tanh'],\n",
    "    'model__dropout_rate': [0.2],\n",
    "    'epochs': [100],\n",
    "    'batch_size': [32]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=clf,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "grid_search.fit(\n",
    "    X_train, y_train,\n",
    "    **{\n",
    "        'callbacks': [early_stop],\n",
    "        'validation_split': 0.2,\n",
    "        'verbose': 1\n",
    "    }\n",
    ")\n"
   ],
   "id": "dfc7909b1661a87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Best params (Grid Search):\", grid_search.best_params_)\n",
    "print(\"Best score (Grid Search):\", grid_search.best_score_)"
   ],
   "id": "251d71e907b426b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Etude du modèle ANN avec les meilleurs hyperparamètres\n",
    "\n",
    "Etude sans scikeras (entièrement avec keras)\n"
   ],
   "id": "8eac6ce1961047d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "best_model = create_ann_model(input_dim=X_train.shape[1],\n",
    "    nb_outputs=len(np.unique(y_train)),\n",
    "    nb_layers=grid_search.best_params_['model__nb_layers'],\n",
    "    nb_neurons=grid_search.best_params_['model__nb_neurons'],\n",
    "    dropout_rate=grid_search.best_params_['model__dropout_rate'],\n",
    "    activation=grid_search.best_params_['model__activation']\n",
    ")\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=grid_search.best_params_['epochs'],\n",
    "    batch_size=grid_search.best_params_['batch_size'],\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ],
   "id": "fe3cde01f3d2a165"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_prob = best_model.predict(X_val)\n",
    "y_pred = y_pred_prob.argmax(axis=1)\n",
    "\n",
    "print(\"Accuracy sur le jeu de validation :\", accuracy_score(y_val, y_pred))"
   ],
   "id": "f5561f31d4898cf1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On constate une très bonne accuracy sur le jeu de validation, ce qui est prometteur pour la classification des déplacements. Les résultats ne sont pas étonnants, au vu des performances des modèles précédents.",
   "id": "504256732e16e875"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(best_model.summary())",
   "id": "e76add56c41606be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Etude de l'entrainement (erreur à chaque epoch) du modèle avec les meilleurs hyperparamètres trouvés.",
   "id": "aa78b8fa9218cc12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Entraînement')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.xlabel(\"Épochs\")\n",
    "plt.ylabel(\"Erreur (loss)\")\n",
    "plt.title(\"Courbe d'apprentissage du meilleur modèle\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "7c77853488de8ee5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On constate que le modèle converge bien, avec une erreur de validation qui diminue au fil des époques. Il n'y a pas de sur-apprentissage évident, ce qui est un bon signe pour la généralisation du modèle. Malgré des meilleurs résultats étonnant sur le jeu de validation que sur le jeu d'entraînement.",
   "id": "58300b961d0d897d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Amélioration du modèle ANN grâce à l'étude de seuil",
   "id": "d59ec2021c8bcf97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Maintenant le but va être de trouver un seuil idéale à partir des sorties des neurones pour augmenter le taux de prédictions neutres, et éviter ainsi des mouvements non souhaités (les minimiser en les classifiant comme neutres).\n",
    "\n",
    "Pour cela nous allons étudier 2 méthodes différentes qui seront détaillés plus bas."
   ],
   "id": "35ebc1eed833e2bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(le.classes_)",
   "id": "bf59e09154e9a505"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Neutre correspond à la classe 4",
   "id": "292c51f1efb3e743"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Les méthodes :",
   "id": "945f706e7c978cfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Méthode A avec delta comme seuil :\n",
    "- pour chaque échantillon, on calcule la différence entre la probabilité maximale (p_max) et la seconde probabilité maximale (p_2nd) et on compare cette différence à un seuil delta. Si (p_max - p_2nd) < delta, on classe l'échantillon comme neutre.\n",
    "\n",
    "Méthode B avec alpha comme seuil :\n",
    "- pour chaque échantillon, on calcule la différence de la probabilité de la classe neutre (p_neutre) et la probabilité de la classe neutre et on la compare à un seuil alpha. Si p_neutre >= alpha, on classe l'échantillon comme neutre."
   ],
   "id": "ee8bf5d756f7cb6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Code généré ici à l'aide chatGPT\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Méthode A\n",
    "delta_list = [0.05, 0.10, 0.20]\n",
    "# Méthode B\n",
    "alpha_list = [0.30, 0.40, 0.50]\n",
    "\n",
    "probas_train = best_model.predict(X_train)\n",
    "\n",
    "print(\"Shape des probas_train :\", probas_train.shape)\n",
    "\n",
    "print(\"Accuracy sur le jeu d'entraînement :\", accuracy_score(y_train, probas_train.argmax(axis=1)))\n",
    "\n",
    "p_neutre_all = probas_train[:, 4]\n",
    "\n",
    "# Pour calculer (p_max - p_2nd) pour chaque échantillon, on fait :\n",
    "# 1) on trie chaque ligne de probas_train pour isoler p_max et p_2nd\n",
    "# 2) on calcule la différence\n",
    "sorted_indices = np.argsort(probas_train, axis=1)\n",
    "# p_sorted[i, -1] = p_max pour l'échantillon i\n",
    "# p_sorted[i, -2] = p_2nd pour l'échantillon i\n",
    "p_sorted = np.take_along_axis(probas_train, sorted_indices, axis=1)\n",
    "p_max_all = p_sorted[:, -1]    # vecteur de taille N_train\n",
    "p_2nd_all = p_sorted[:, -2]    # vecteur de taille N_train\n",
    "\n",
    "delta_all = p_max_all - p_2nd_all  # vecteur (p_max - p_2nd) pour chaque échantillon\n",
    "\n",
    "# --- 2.4. Boucle sur toutes les combinaisons (delta, alpha) ---\n",
    "# On va construire une liste de lignes pour un DataFrame, où chaque ligne contiendra :\n",
    "#    [delta, alpha, accuracy]\n",
    "records = []\n",
    "\n",
    "for delta in delta_list:\n",
    "    for alpha in alpha_list:\n",
    "        # Pour chaque échantillon i, on décide la prédiction finale selon :\n",
    "        #   si p_neutre >= alpha  OU  delta_i < delta  => classe 2 (neutre)\n",
    "        #   sinon                                      => argmax(probas_train[i])\n",
    "        # On peut vectoriser cette logique en deux étapes :\n",
    "\n",
    "        # Étape 1 : on initialise pred = argmax(probas_train, axis=1)\n",
    "        preds_argmax = np.argmax(probas_train, axis=1)\n",
    "\n",
    "        # Étape 2 : on marque en 2 (neutre) tous les indices i où\n",
    "        #           p_neutre_all[i] >= alpha   OU   delta_all[i] < delta\n",
    "        # On crée un masque booléen \"to_neutral\" de taille N_train :\n",
    "        to_neutral = np.logical_or(p_neutre_all >= alpha, delta_all < delta)\n",
    "\n",
    "        # On remplace dans preds_argmax(i) par 2 si to_neutral[i] est True\n",
    "        final_preds = preds_argmax.copy()\n",
    "        final_preds[to_neutral] = 4\n",
    "\n",
    "        # --- 2.5. Calcul de l’accuracy sur X_train/y_train pour ce (delta, alpha) ---\n",
    "        acc = accuracy_score(y_train, final_preds)\n",
    "\n",
    "        # On stocke le triplet\n",
    "        records.append({\n",
    "            'delta': delta,\n",
    "            'alpha': alpha,\n",
    "            'accuracy': acc\n",
    "        })\n",
    "\n",
    "# --- 2.6. Construction du DataFrame 3×3 « matrice » ---\n",
    "df_results = pd.DataFrame(records)\n",
    "\n",
    "# Pivot : ligne = delta, colonne = alpha, valeur = accuracy\n",
    "matrix_acc = df_results.pivot(index='delta', columns='alpha', values='accuracy')\n",
    "\n",
    "# Affichage\n",
    "print(\"Matrice des accuracies (rows = delta, cols = alpha) :\\n\")\n",
    "print(matrix_acc)\n"
   ],
   "id": "3682d27554a5e06f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On remarque étonnament que les seuils améliorent l'accuracy du modèle, ce qui n'était pas attendu. En effet, on s'attendait à ce que les seuils réduisent l'accuracy en classifiant des échantillons comme neutres alors qu'ils ne le sont pas.",
   "id": "9c5b99f9899bed12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On conservera le seuil delta = 0.20 et alpha = 0.30, car il permet d'augmenter l'accuracy du modèle tout en réduisant les mouvements non souhaités. On les réutilisera dans model_API_ANN.py pour la classification des mouvements.",
   "id": "a6b5d96e46297d77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Sauvegarde du meilleur modèle qui sera utilisé dans l'API model_API_ANN.py, ainsi que du label encoder pour la classification des mouvements.",
   "id": "fd6be5948d92517c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import joblib\n",
    "#\n",
    "# best_model.save('../models_ANN/best_ann_model_move.h5')\n",
    "# joblib.dump(le, \"../models_ANN/label_encoder_move.pkl\")"
   ],
   "id": "a32cdaa9148d4ba7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finalement je ne vais pas étudier la position 2 pour la vitesse de la même façon que le neutre du mouvement, cela ne servirait à rien car la position 2 n'est pas vraiment considéré comme une position neutre.",
   "id": "2cba4675c4d7d544"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
